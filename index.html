<!DOCTYPE html>
<html>
  <head>
    <title>Umberto Cappellazzo</title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="PhD @ UniTn, CL for audio and speech">
    <meta property="og:description" content="PhD @ UniTn, CL for audio and speech"/>
    
    <meta name="author" content="Umberto Cappellazzo" />

  
    <link rel="stylesheet" type="text/css" href="style.css" />
    <link rel="shortcut icon" type="image/x-icon" href="/images/image.ico"/>

    <style>

mark{
 background-color:#c0ffc8;
}

.border-highlight{
  border:2px dashed red;  
  padding:0.03em 0.25em;
}

li{
  margin: 10px 0;
}
    </style>

  </head>

  <body>
    <div class="wrapper-masthead">
      <div class="container">

        <header class="masthead clearfix">

          <div class="site-info">
            <h1 class="site-name"><a href="/">Umberto Cappellazzo</a></h1>
            <p class="site-description">Research Associate @ Imperial College London.</p>
          </div>


          <nav>
            <a href="/#news">News</a>
            <a href="/#publications">Publications</a>
            <a href="/#work_experience">Work Experience</a>
            <a href="/#education">Education</a>
            <a href="CV/Resume.pdf">CV</a>
          </nav>
        </header>

      </div>
    </div>

    <div id="main" role="main" class="container">
      <div class="portfolio_text">
<img src="./images/profile.jpeg" alt="Umberto's profile photo" width="28%">

<div class="sample-text">
    <span>
    Howdy! I'm Umberto Cappellazzo, and I work as a Research Associate in the Department of Computing at Imperial College London, UK. I'm a member of the iBUG group led by Maja Pantic and am fortunate to be advised by Stavros Petridis. My research focuses on the parameter-efficient massive scaling of audio-visual models using Mixture of Experts and Large Language Models. Previously, I obtained my PhD in Information Engineering and Computer Science from the University of Trento, Italy. During my PhD, I explored diverse topics, including continual learning for speech processing, parameter-efficient fine-tuning techniques (e.g, adapters, LoRA) for audio/speech tasks, and Multimodal LLMs for audio-visual speech recognition, leading to nine publications in top-notch conferences. During the final year of my PhD, I spent nine months as a visiting researcher with the iBUG team at Imperial. 

  
    </span>
</div>
  <div class="icon">
    <a href="mailto:umbertocappellazzo@gmail.com"><img src="./images/email_icon.png" width="24px"></a>
    <a href="https://www.linkedin.com/in/umberto-cappellazzo-116093150/"><img src="./images/linkedin_icon.png" width="24px"></a>
    <a href="https://scholar.google.com/citations?user=z7zK5S0AAAAJ&hl=en"><img src="./images/google_scholar_icon.png" width="24px"></a>
    <a href="https://github.com/umbertocappellazzo"><img src="./images/github_icon.png" width="24px"></a>
  </div>
</div>

<h2 id="news">News</h2>

<ul>
              <li>
                <b> 19/09/2025.</b> 
                I'm super excited to share that our paper "<a href="/#MoME"><span style="font-family:'Comic Sans MS'">MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition</span>"</a> has been accepted to <b><mark style="background-color: #FFF0F5;">NeurIPS 2025</mark></b>! The camera-ready version is available <a href="" target="_blank">here</a>. We introduce <i>Mixture of Matryoshka Experts</i> (MoME), which unifies Matryoshka Representation Learning with sparse Mixture-of-Experts for AVSR. MoME augments frozen LLMs with top-k routed and shared experts, enabling dynamic capacity allocation across modalities and granularities while capturing global, cross-modal, and scale-invariant knowledge.
                
              </li>

              <li>
                <b> 07/08/2025.</b> 
                Delighted to announce that our paper "<a href="/#Llama-MTSK"><span style="font-family:'Comic Sans MS'">Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal LLMs</span>"</a> has been accepted to <b><mark style="background-color: #B0C4DE;">IEEE ASRU 2025</mark></b>! The camera-ready version is available <a href="https://arxiv.org/abs/2503.06362" target="_blank">here</a>. 
                One model, elastic inference, and strong performance across multiple tasks via <u>parameter-efficient</u> <b>matryoshka representation learning</b>.
                I'll be sharing further details in Honolulu, Hawaii this December! ðŸŒº
              </li>

              <li>
                <b> 22/05/2025.</b> 
                Super happy to share that our paper "<a href="/#Llama-SMoP"><span style="font-family:'Comic Sans MS'">Scaling and Enhancing LLM-based AVSR: A Sparse Mixture of Projectors Approach</span>"</a> has been accepted at <b><mark style="background-color: #DCDCDC;">Interspeech 2025</mark></b>!! The camera-ready version is available <a href="https://arxiv.org/abs/2505.14336" target="_blank">here</a>. We show how to effectively apply <b>Mixture of Experts</b> (MoE) to the projector layers for <b>LLM-based AVSR</b> (i.e., <b>Llama-SMoP</b>). Simple, efficient, and model-agnostic, yay! Looking forward to sharing more details in Rotterdam this August!
              </li>

              <li>
                <b> 11/03/2025.</b> 
                I'm stoked to announce that I've joined the Department of Computing at <b>Imperial College London</b> with the iBUG team (led by Maja Pantic) as a research associate. I'm fortunate to be advised by Stavros Petridis. My research focus is on the efficient scaling of audio-visual models via Mixture of Experts and LLMs. Ad maiora! 
              </li>

              <li>
                <b> 15/01/2025.</b> 
                
                Extremely proud and happy to announce that on January 15, 2025 I successfully defended my PhD "cum laude" at the University of Trento, Italy. What an incredible journey my 3-year PhD has been, full of experiences, collaborations, and achievements. If interested, <a href="PhD_Thesis/Thesis_PhD.pdf" target="_blank">here</a> you can read my <u><mark style="background-color: #D8BFD8;"><span style="font-family:'Comic Sans MS'">final PhD dissertation</span></mark></u>, where <a href="https://docs.google.com/presentation/d/15nzONJ3Bf1E70-8frrh8wvZXfW2hnujjLzDBhneUC_A/edit?usp=sharing" target="_blank">here</a> you can have access to my <u><mark style="background-color: #D8BFD8;"><span style="font-family:'Comic Sans MS'">PhD Defense slides</span></mark></u>.
                
              </li>
              
              <li>
                <b> 20/12/2024.</b> 
                
                Delighted to announce that our paper "<a href="/#Llama-AVSR"><span style="font-family:'Comic Sans MS'">Large Language Models are Strong Audio-Visual Speech Recognition Learners</span>"</a> has been accepted at <b><mark style="background-color: #FF7F50;">ICASSP 2025</mark></b>. The camera-ready version will be uploaded <a href="https://arxiv.org/abs/2409.12319" target="_blank">here</a> soon.

                
              </li>

                <li>
                <b> 28/09/2024.</b> 
                
                Introducing <mark style="background-color: #FF7F50;">Llama-AVSR</mark>, a multimodal LLM with strong audio-visual speech recognition capabilities. 
                We attain new state-of-the-art results on the LRS3 benchmark, the largest public AVSR benchmark, for the tasks of ASR and AVSR, and sota results for the task of VSR. We also unveil the key factors that lead to the effectiveness of Llama-AVSR: the choice of the pre-trained encoders and LLMs, the use of LoRA modules and how to achieve the optimal performance-efficiency trade-off via modality-aware compression rates.

                This work has been done while visiting Imperial College London ðŸ‡¬ðŸ‡§ in collaboration with Meta AI.

                More details about the pre-print <a href="https://arxiv.org/abs/2409.12319" target="_blank">here</a>.

                
              </li>
              <li>
                <b> 16/07/2024.</b> 
                Excited to share that my paper <i><b><a href="/#PEFT_AST">Parameter-Efficient Transfer Learning of Audio Spectrogram Transformers</a></b></i> has been accepted at the <mark style="background-color: #F5BC17;">IEEE International Workshop on Machine Learning for Signal Processing (MLSP) 2024</mark>. The workshop will take place in September 22-25, 2024 at Imperial College London, London, UK. The paper investigates the efficient fine-tuning of the Audio Spectrogram Transformer model to multiple audio and speech tasks. We take into account various PEFT methods like bottleneck adapter, prompt-tuning, and LoRA. Furthermore, we propose a new adapter module that leverages the convolution module of the conformer architecture. Our experiments reveal that our <b><mark style="background-color: #CC99FF;">conformer adapter</mark></b> outstrips the other PEFT methods and is able to surpass or achieve performance parity with full fine-tuning by updating only 0.29% of the parameters. We also show that <b>1)</b> the conformer adapter is effective in few-shot efficient transfer learning, <b>2)</b> attains optimal results regardless of the amount of the allocated parameters, and <b>3)</b> can be applied to other pre-trained models like Wav2Vec 2.0.
              </li>

              <li>
                <b> 04/06/2024.</b> 
                So glad to announce the recent acceptance of <u>three</u> papers of mine &#128640 &#128640 &#128640!  <b>1)</b> "<a href="/#continual_contrastive">Continual Contrastive Spoken Language Understanding</a>" has been accepted to <mark style="background-color: #EE82EE;">ACL Findings 2024</mark>! <b>2)</b> "<a href="/#soft_moa">Efficient Fine-tuning of Audio Spectrogram Transformers via Soft Mixture of Adapters"</a> and <b>3)</b> "<a href="/#towards_unified">Towards Unified Evaluation of Continual Learning in Spoken Language Understanding"</a> (the camera-ready version will be soon released) have been accepted to <mark style="background-color: #66CDAA;">Interspeech 2024!</mark>
              </li>
              <li>
                <b> 22/04/2024.</b> 
                Excited to announce that on April 22 I gave a talk @ University of Cambridge for the CUED Speech Group Seminars. The title of the talk is "Parameter-Efficient Fine-tuning for Audio and Speech Processing". The first part of the talk investigates the use of PEFT methods (e.g., LoRA, adapters, prompt-tuning) for audio and speech downstream tasks, with a focus on the efficient adaptation of the Audio Spectrogram Transformer. In the second part I presented two recent works of mine to boost PEFT performance based on Soft Mixture of Adapters and new ad-hoc adapter designs. 
              </li>
              <li>
                <b> 20/02/2024.</b> 
                &#128227 &#128227 Excited to announce that I've officially joined the department of computing at Imperial College London, London as a Visiting PhD researcher &#128640. I'll be exploring the use of LLMs for audio-visual speech recognition &#128293. I'm fortunate to be advised by Stavros Petridis and Pingchuan Ma from Meta AI.
              </li>

              <li>
                <b> 02/02/2024.</b> 
                Excited to share a new pre-print: <i><b>Efficient Fine-tuning of Audio Spectrogram Transformers via Soft Mixture of Adapters</b></i>. In this <a href="https://arxiv.org/abs/2402.00828" target="_blank">paper</a> we propose the use of <b><mark style="background-color: #FF4500;">Soft-MoA</mark></b> for the efficient fine-tuning of the AST model, leveraging a recent approach by Google DeepMind (Soft MoE). We test Soft-MoA in 4 audio and speech benchmarks, showcasing that it attains competitive performance with respect to the dense counterpart, while drastically trimming down the computational cost. We also demonstrate that Soft-MoA outperforms the single adapter approach. We finally perform multiple ablation studies on pivotal aspects of Soft-MoA. This paper is the second paper of a trilogy &#127916: the first <a href="/#PETL">paper</a> provides a birds's eye overview of PETL methods for AST, while the third and last one will be released in the next few months, so get ready for the last chapter! &#128378 


              </li>



              <li>
                  <b> 03/09/2023.</b> 
                  I've started collecting the most interesting recent papers pertaining to Continual Learning. Categorized based on the their domain (<i>audio</i>, <i>text</i>, <i>vision</i>) and on the methods employed (<i>regularization</i>, <i>rehearsal</i>, <i>architectural</i>), the papers you can find in this <a href="https://github.com/umbertocappellazzo/CL_Anthology" target="_blank">github repo</a> provide a bird's-eye view of what "learning in a sequential fashion" truly refers to. Willing to accept any PR on a compelling paper worth being included in this compendium! 

              </li>

              <li>
                  <b> 06/08/2023.</b> The curtain has fallen on JSALT 2023! It's been a wonderful experience, working together with several researchers towards the same objective: <b><mark style="background-color: #CC99FF;">make FST great again</mark></b>! I worked on the integration of early-exit techniques to make the training and inference of CTC/MMI systems dynamical. If you want to know more about my work and that of my colleagues, you can watch the saved <a href="https://www.youtube.com/watch?v=O9vw5TDZ2Ug" target="_blank">YouTube live stream</a> of the last day where my group discussed all our achievements! 

              </li>

              <li>
                  <b> 23/05/2023.</b> Super happy to share that I will join <a href="https://jsalt2023.univ-lemans.fr/en/index.html" target="_blank">JSALT 2023</a> in Le Mans, France this summer! I will be part of the "<i>Finite state methods with modern neural Architectures for speech applications and beyond</i> " group. I will be working with people from Google, Telecom Paris, JHU, among others. 

              </li>

              <li>
                  <b> 20/05/2023.</b>   Both my papers have been accepted at <b><mark>INTERSPEECH 2023</mark></b>! &#128640&#128640 See you in Dublin! &#127808 <br /> 
                  <strong>1)</strong> <i>"An Investigation of the Combination of Rehearsal and Knowledge Distillation in Continual Learning For Spoken Language Understanding"</i>. <br />
                  <strong>2)</strong> <i>"Sequence-Level Knowledge Distillation for Class-Incremental End-to-End Spoken Language Understanding"</i>.  
              </li>
              
            </ul>



<h2 id="publications">Publications</h2>

<br>

<div id="PETL">
      <img class="img-fluid" src="./images/MoME.png", width="291" height="119">
      <div class="portfolio_text">
        <div class="paper_title" style="font-family:'Comic Sans MS'" id="MoME"><i>MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition</i></div>
        <div class="authors"><strong>U. Cappellazzo</strong>, M. Kim, P. Ma, H. Chen, X. Liu, S. Petridis, M. Pantic
        </div>
        <div class="submission_info">
          <mark style="background-color: #FFF0F5;">NeurIPS 2025</mark>
        </div>
        <div class="link">
          <a href="">Paper</a>
        </div>
      </div>
</div>

<br>

<div id="PETL">
      <img class="img-fluid" src="./images/Llama-MTSK.png", width="291" height="150">
      <div class="portfolio_text">
        <div class="paper_title" style="font-family:'Comic Sans MS'" id="Llama-MTSK"><i>Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal LLMs</i></div>
        <div class="authors"><strong>U. Cappellazzo</strong>, M. Kim, S. Petridis
        </div>
        <div class="submission_info">
          <mark style="background-color: #B0C4DE;">IEEE ASRU 2025</mark>
        </div>
        <div class="link">
          <a href="https://arxiv.org/abs/2503.06362">Paper</a>
        </div>
      </div>
</div>

<br>
<br>

<div id="PETL">
      <img class="img-fluid" src="./images/Llama_SMoP.png", width="350" height="100">
      <div class="portfolio_text">
        <div class="paper_title" style="font-family:'Comic Sans MS'" id="Llama-SMoP"><i>Scaling and Enhancing LLM-based AVSR: A Sparse Mixture of Projectors Approach</i></div>
        <div class="authors"><strong>U. Cappellazzo</strong>, M. Kim, S. Petridis, D. Falavigna, A. Brutti
        </div>
        <div class="submission_info">
          <mark style="background-color: #DCDCDC;">Interspeech 2025</mark>
        </div>
        <div class="link">
          <a href="https://arxiv.org/abs/2505.14336">Paper</a>
        </div>
      </div>
</div>

<br>

<div id="PETL">
      <img class="img-fluid" src="./images/Llama-AVSR.png", width="222" height="200">
      <div class="portfolio_text">
        <div class="paper_title" style="font-family:'Comic Sans MS'" id="Llama-AVSR"><i>Large Language Models Are Strong Audio-Visual Speech Recognition Learners</i></div>
        <div class="authors"><strong>U. Cappellazzo</strong>, M. Kim, H. Chen, P. Ma, S. Petridis, D. Falavigna, A. Brutti, M. Pantic
        </div>
        <div class="submission_info">
          <mark style="background-color: #FF7F50;">ICASSP 2025</mark>
        </div>
        <div class="link">
          <a href="https://arxiv.org/abs/2409.12319">Paper</a>
          <a href="https://github.com/umbertocappellazzo/Llama-AVSR">Code</a>&emsp;
        </div>
      </div>
</div>

<br>
<br>
<br>

<div id="PETL">
      <img class="img-fluid" src="./images/AST_main.png", width="160" height="200">
      <div class="portfolio_text">
        <div class="paper_title" style="font-family:'Comic Sans MS'" id="PEFT_AST"><i>Parameter-Efficient Transfer Learning of Audio
        Spectrogram Transformers</i></div>
        <div class="authors"><strong>U. Cappellazzo</strong>, D. Falavigna, A. Brutti, M. Ravanelli
        </div>
        <div class="submission_info">
          <mark style="background-color: #F5BC17;">IEEE MLSP Workshop, 2024</mark>
        </div>
        <div class="link">
          <a href="https://arxiv.org/abs/2312.03694">Paper</a>
          <a href="https://github.com/umbertocappellazzo/PETL_AST">Code</a>&emsp;
        </div>
      </div>
</div>

<br>
<br>
<br>
<br>

<div>
      <img class="img-fluid" src="./images/coconut.jpeg", width="300" height="100">
      <div class="portfolio_text">
        <div class="paper_title" style="font-family:'Comic Sans MS'" id="continual_contrastive"><i>Continual Contrastive Spoken Language Understanding</i></div>
        <div class="authors"><strong>U. Cappellazzo</strong>, E. Fini, M. Yang, D. Falavigna, A. Brutti, B. Raj
        </div>
        <div class="submission_info">
          <mark style="background-color: #EE82EE;">ACL Findings 2024</mark>
        </div>
        <div class="link">
          <a href="https://arxiv.org/abs/2310.02699">Paper</a>
        </div>
      </div>
</div>

<br>

<div>
      <img class="img-fluid" src="./images/MoA.png", width="300" height="110">
      <div class="portfolio_text">
        <div class="paper_title" style="font-family:'Comic Sans MS'" id="soft_moa"><i>Efficient Fine-tuning of Audio Spectrogram Transformers via Soft Mixture of Adapters</i></div>
        <div class="authors"><strong>U. Cappellazzo</strong>, D. Falavigna, A. Brutti
        </div>
        <div class="submission_info">
          <mark style="background-color: #66CDAA;">Interspeech, 2024</mark>
        </div>
        <div class="link">
          <a href="https://arxiv.org/abs/2402.00828">Paper</a>
          <a href="https://github.com/umbertocappellazzo/PETL_AST">Code</a>&emsp;
        </div>
      </div>
</div>


<br>
<div>
      <img class="img-fluid" src="./images/new_index.png", width="200" height="192">
      <div class="portfolio_text">
        <div class="paper_title" style="font-family:'Comic Sans MS'" id="towards_unified"><i>Evaluating and Improving Continual Learning in Spoken Language Understanding</i></div>
        <div class="authors">M. Yang, X. Li, <strong>U. Cappellazzo</strong>, S. Watanabe, B. Raj
        </div>
        <div class="submission_info">
          <mark style="background-color:#66CDAA ;">Interspeech, 2024</mark>
        </div>
        <div class="link">
          <a href="https://arxiv.org/abs/2402.10427">Paper</a>
        </div>
      </div>
</div>

<br>
<br>
<br>
<br>

<div>
      <img class="img-fluid" src="./images/CL_MI.png", width="250" height="153">
      <div class="portfolio_text">
        <div class="paper_title" style="font-family:'Comic Sans MS'"><i>Improving continual learning of acoustic scene classification via mutual information optimization</i></div>
        <div class="authors">M. Yang, <strong>U. Cappellazzo</strong>, X. Li, S. Watanabe, B. Raj
        </div>
        <div class="submission_info">
          <mark style="background-color: #87CEEB;">ICASSP 2024</mark>
        </div>
        <div class="link">
          <a href="https://ieeexplore.ieee.org/abstract/document/10446846">Paper</a>
        </div>
      </div>
</div>

<br>
<br>
<br>

<div>
      <img class="img-fluid" src="./images/EE_SS.png", width="300" height="131">
      <div class="portfolio_text">
        <div class="paper_title" style="font-family:'Comic Sans MS'"><i>Training Dynamic Models using Early Exits for Automatic Speech Recognition on Resource-constrained Devices</i></div>
        <div class="authors"> G. A. Wright, <strong>U. Cappellazzo</strong>, S. Zaiem, D. Raj, L. Ondel Yang,  D. Falavigna, M. Ali, A. Brutti
        </div>
        <div class="submission_info">
          <mark style="background-color: #FFEBCD;">Self-supervision in Audio, Speech and Beyond Workshop, ICASSP 2024</mark>
        </div>
        <div class="link">
          <a href="https://arxiv.org/abs/2309.09546">Paper</a>&emsp;
          <a href="https://github.com/augustgw/early-exit-transformer">Code</a>&emsp;
        </div>
      </div>
</div>

<br>
<div>
      <img class="img-fluid" src="./images/IS_slurp.png", width="300" height="165">
      <div class="portfolio_text">
        <div class="paper_title" style="font-family:'Comic Sans MS'"><i>Sequence-Level Knowledge Distillation for Class-Incremental End-to-End Spoken Language Understanding</i></div>
        <div class="authors"> <strong>Umberto Cappellazzo</strong>, Muqiao Yang, Daniele Falavigna, Alessio Brutti
        </div>
        <div class="submission_info">
          <mark style="background-color: #CC99FF;">INTERSPEECH 2023 (Oral)</mark>
        </div>
        <div class="link">
          <a href="https://arxiv.org/abs/2305.13899">Paper</a>&emsp;
          <a href="https://github.com/umbertocappellazzo/SLURP-SeqKD">Code</a>&emsp;
        </div>
      </div>
</div>

<br>
<div>
      <img class="img-fluid" src="./images/FSC_CL.png", width="300" height="180">
      <div class="portfolio_text">
        <div class="paper_title" style="font-family:'Comic Sans MS'">An Investigation of the Combination of Rehearsal and Knowledge Distillation in Continual Learning for Spoken Language Understanding
        </div>
        <div class="authors"> <strong>Umberto Cappellazzo</strong>, Daniele Falavigna, Alessio Brutti
        </div>
        <div class="submission_info">
          <mark style="background-color: #CC99FF;">INTERSPEECH 2023 (Poster)</mark>
        </div>
        <div class="link">
          <a href="https://arxiv.org/abs/2211.08161">Paper</a>&emsp;
          <a href="https://github.com/umbertocappellazzo/CL_SLU">Code</a>&emsp;
        </div>
      </div>
</div>
<br>


<h2 id="work_experience">Work Experience</h2>
<ul>
  <li><span><img src="./images/Imperial_logo.jpg" width="20px"></span>
  Research Associate in the iBUG group<br>
  Imperial College London, London, UK<br>
  Mar 2025-ongoing<br>
  <b>Research interests</b>: <i>Efficient massive scaling of audio-visual models, Mixture of Experts, Multimodal LLMs</i> <br>
  <b>Advisor</b>: Stavros Petridis<br>
  </li>

</ul>

<h2 id="education">Education</h2>
<ul>
  <li><span><img src="./images/unitn_logo.png" width="20px"></span>
  PhD in Information Engineering and Computer Science  <br>
  University of Trento, Trento, Italy<br>
  Nov 2021-Jan 2025<br>
  <b>Title</b>: <i>Efficient Knowledge Transfer and Adaptation for Speech and Beyond</i> <br>
  <b>Supervisors</b>: Daniele Falavigna, Alessio Brutti<br>
  </li>

  <li>
    <span><img src="./images/logo_unipd.png" width="20px"></span>
  M.S. in Telecommunication Engineering<br>
  University of Padova, Padova, Italy<br>
  2016-2019<br>
  <b>Thesis title</b>: <i>A Deep Learning-Based ECG Delineator: Evaluation and Comparison on Standard Databases</i>. 
  <br>
  <b>Supervisors</b>: Michele Rossi, Matteo Gadaleta
  </li>

  <li>
    <span><img src="./images/logo_unipd.png" width="20px"></span>
  B.S. in Information Engineering<br>
  University of Padova, Padova, Italy<br>
  2013-2016<br>
  <b>Thesis title</b>: <i>Message Authentication over an Ideal or Noisy Channel</i> 
  <br>
  <b>Supervisor</b>: Nicola Laurenti
  </li>

</ul>


<h2>Contact</h2>
Feel free to reach out to me if you have any questions upon my research activity. Plus, I'm always apt to new collaborations! Please contact me at <i>umbertocappellazzo [at] gmail [dot] com</i>.
    
</br>
</br>

  </body>
</html>