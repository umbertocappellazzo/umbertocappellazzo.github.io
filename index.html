<!DOCTYPE html>
<html>
  <head>
    <title>Umberto Cappellazzo</title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="PhD @ UniTn, CL for audio and speech">
    <meta property="og:description" content="PhD @ UniTn, CL for audio and speech"/>
    
    <meta name="author" content="Umberto Cappellazzo" />

  
    <link rel="stylesheet" type="text/css" href="style.css" />
    <link rel="shortcut icon" type="image/x-icon" href="/images/image.ico"/>

    <style>

mark{
 background-color:#c0ffc8;
}

.border-highlight{
  border:2px dashed red;  
  padding:0.03em 0.25em;
}

li{
  margin: 10px 0;
}
    </style>

  </head>

  <body>
    <div class="wrapper-masthead">
      <div class="container">

        <header class="masthead clearfix">

          <div class="site-info">
            <h1 class="site-name"><a href="/">Umberto Cappellazzo</a></h1>
            <p class="site-description">PhD student @ UniTn on CL for audio and speech</p>
          </div>


          <nav>
            <a href="/#news">News</a>
            <a href="/#publications">Publications</a>
            <a href="/#education">Education</a>
            <a href="CV/Resume.pdf">CV</a>
          </nav>
        </header>

      </div>
    </div>

    <div id="main" role="main" class="container">
      <div class="portfolio_text">
<img src="./images/profile.jpeg" alt="Umberto's profile photo" width="28%">

<div class="sample-text">
    <span>
    Howdy! I'm Umberto Cappellazzo, a PhD candidate at the University of Trento, Italy, advised by Daniele Falavigna and Alessio Brutti. I'm a member of the <a href="https://speechtek.fbk.eu/" target="_blank">SpeechTek</a> group at Fondazione Bruno Kessler (FBK), Trento. My research activity is about <b>Continual Learning </b> (CL) for <b>speech-related</b> tasks, such as <i>Spoken Language Understanding</i> (SLU) and <i>Automatic Speech Recognition</i> (ASR), and the study of <i>Parameter-Efficient Transfer Learning</i> (PETL) techniques (e.g, adapters, LoRA) for audio tasks. I'm also interested in multi-modal audio-text system and in the exploration of <span class="border-highlight">Mixture of Models (MoE)</span> for audio classification. I'm currently embarking on a new project about the integration of LLms for audio-visual learning tasks <b><mark style="background-color: #FFEBCD;">("Audio-visual Speech Recognition Meets LLMs")</mark></b> @ Imperial College London under the supervision of Stavros Petridis and Pingchuan Ma from Meta AI.
    </span>
</div>
  <div class="icon">
    <a href="mailto:umbertocappellazzo@gmail.com"><img src="./images/email_icon.png" width="24px"></a>
    <a href="https://www.linkedin.com/in/umberto-cappellazzo-116093150/"><img src="./images/linkedin_icon.png" width="24px"></a>
    <a href="https://scholar.google.com/citations?user=z7zK5S0AAAAJ&hl=en"><img src="./images/google_scholar_icon.png" width="24px"></a>
    <a href="https://github.com/umbertocappellazzo"><img src="./images/github_icon.png" width="24px"></a>
  </div>
</div>

<h2 id="news">News</h2>

<ul>
              <li>
                <b> 20/02/2023.</b> 
                &#128227 &#128227 Excited to announce that I've officially joined the department of computing at Imperial College London, London for a 3-months visiting period &#128640. I'll be exploring the use of LLMs for audio-visual speech recognition &#128293. I'm fortunate to be advised by Stavros Petridis and Pingchuan Ma from Meta AI.
              </li>

              <li>
                <b> 02/02/2023.</b> 
                Excited to share a new pre-print: <i><b>Efficient Fine-tuning of Audio Spectrogram Transformers via Soft Mixture of Adapters</b></i>. In this <a href="https://arxiv.org/abs/2402.00828" target="_blank">paper</a> we propose the use of <b><mark style="background-color: #FF4500;">Soft-MoA</mark></b> for the efficient fine-tuning of the AST model, leveraging a recent approach by Google DeepMind (Soft MoE). We test Soft-MoA in 4 audio and speech benchmarks, showcasing that it attains competitive performance with respect to the dense counterpart, while drastically trimming down the computational cost. We also demonstrate that Soft-MoA outperforms the single adapter approach. We finally perform multiple ablation studies on pivotal aspects of Soft-MoA. This paper is the second paper of a trilogy &#127916: the first <a href="/#PETL">paper</a> provides a birds's eye overview of PETL methods for AST, while the third and last one will be released in the next few months, so get ready for the last chapter! &#128378 


              </li>



              <li>
                  <b> 03/09/2023.</b> 
                  I've started collecting the most interesting recent papers pertaining to Continual Learning. Categorized based on the their domain (<i>audio</i>, <i>text</i>, <i>vision</i>) and on the methods employed (<i>regularization</i>, <i>rehearsal</i>, <i>architectural</i>), the papers you can find in this <a href="https://github.com/umbertocappellazzo/CL_Anthology" target="_blank">github repo</a> provide a bird's-eye view of what "learning in a sequential fashion" truly refers to. Willing to accept any PR on a compelling paper worth being included in this compendium! 

              </li>

              <li>
                  <b> 06/08/2023.</b> The curtain has fallen on JSALT 2023! It's been a wonderful experience, working together with several researchers towards the same objective: <b><mark style="background-color: #CC99FF;">make FST great again</mark></b>! I worked on the integration of early-exit techniques to make the training and inference of CTC/MMI systems dynamical. If you want to know more about my work and that of my colleagues, you can watch the saved <a href="https://www.youtube.com/watch?v=O9vw5TDZ2Ug" target="_blank">YouTube live stream</a> of the last day where my group discussed all our achievements! 

              </li>

              <li>
                  <b> 23/05/2023.</b> Super happy to share that I will join <a href="https://jsalt2023.univ-lemans.fr/en/index.html" target="_blank">JSALT 2023</a> in Le Mans, France this summer! I will be part of the "<i>Finite state methods with modern neural Architectures for speech applications and beyond</i> " group. I will be working with people from Google, Telecom Paris, JHU, among others. 

              </li>

              <li>
                  <b> 20/05/2023.</b>   Both my papers have been accepted at <b><mark>INTERSPEECH 2023</mark></b>! &#128640&#128640 See you in Dublin! &#127808 <br /> 
                  <strong>1)</strong> <i>"An Investigation of the Combination of Rehearsal and Knowledge Distillation in Continual Learning For Spoken Language Understanding"</i>. <br />
                  <strong>2)</strong> <i>"Sequence-Level Knowledge Distillation for Class-Incremental End-to-End Spoken Language Understanding"</i>.  
              </li>
              
            </ul>



<h2 id="publications">Publications</h2>

<br>
<div>
      <img class="img-fluid" src="./images/new_index.png", width="200" height="192">
      <div class="portfolio_text">
        <div class="paper_title" style="font-family:'Comic Sans MS'"><i>Evaluating and Improving Continual Learning in Spoken Language Understanding</i></div>
        <div class="authors">M. Yang, X. Li, <strong>U. Cappellazzo</strong>, S. Watanabe, B. Raj
        </div>
        <div class="submission_info">
          <mark style="background-color:#F5BC17 ;">arxiv preprint, 2024</mark>
        </div>
        <div class="link">
          <a href="https://arxiv.org/abs/2402.10427">Paper</a>
        </div>
      </div>
</div>

<br>
<br>
<br>
<div>
      <img class="img-fluid" src="./images/CL_MI.png", width="250" height="153">
      <div class="portfolio_text">
        <div class="paper_title" style="font-family:'Comic Sans MS'"><i>Improving continual learning of acoustic scene classification via mutual information optimization</i></div>
        <div class="authors">M. Yang, <strong>U. Cappellazzo</strong>, X. Li, S. Watanabe, B. Raj
        </div>
        <div class="submission_info">
          <mark style="background-color: #87CEEB;">ICASSP 2024</mark>
        </div>
        <div class="link">
          <a href="https://openreview.net/pdf?id=bWDcAidgC9">Paper</a>
        </div>
      </div>
</div>

<br>
<br>
<div>
      <img class="img-fluid" src="./images/MoA.png", width="300" height="110">
      <div class="portfolio_text">
        <div class="paper_title" style="font-family:'Comic Sans MS'"><i>Efficient Fine-tuning of Audio Spectrogram Transformers via Soft Mixture of Adapters</i></div>
        <div class="authors"><strong>U. Cappellazzo</strong>, D. Falavigna, A. Brutti
        </div>
        <div class="submission_info">
          <mark style="background-color: #F5BC17;">arxiv preprint, 2024</mark>
        </div>
        <div class="link">
          <a href="https://arxiv.org/abs/2402.00828">Paper</a>
          <a href="https://github.com/umbertocappellazzo/PETL_AST">Code</a>&emsp;
        </div>
      </div>
</div>

<br>
<div id="PETL">
      <img class="img-fluid" src="./images/AST_main.png", width="160" height="200">
      <div class="portfolio_text">
        <div class="paper_title" style="font-family:'Comic Sans MS'"><i>Parameter-Efficient Transfer Learning of Audio
        Spectrogram Transformers</i></div>
        <div class="authors"><strong>U. Cappellazzo</strong>, D. Falavigna, A. Brutti, M. Ravanelli
        </div>
        <div class="submission_info">
          <mark style="background-color: #F5BC17;">arxiv preprint, 2023</mark>
        </div>
        <div class="link">
          <a href="https://arxiv.org/abs/2312.03694">Paper</a>
          <a href="https://github.com/umbertocappellazzo/PETL_AST">Code</a>&emsp;
        </div>
      </div>
</div>

<br>
<br>
<br>
<br>

<div>
      <img class="img-fluid" src="./images/coconut.jpeg", width="300" height="100">
      <div class="portfolio_text">
        <div class="paper_title" style="font-family:'Comic Sans MS'"><i>Continual Contrastive Spoken Language Understanding</i></div>
        <div class="authors"><strong>U. Cappellazzo</strong>, E. Fini, M. Yang, D. Falavigna, A. Brutti, B. Raj
        </div>
        <div class="submission_info">
          <mark style="background-color: #F5BC17;">arxiv preprint, 2023</mark>
        </div>
        <div class="link">
          <a href="https://arxiv.org/abs/2310.02699">Paper</a>
        </div>
      </div>
</div>

<br>
<div>
      <img class="img-fluid" src="./images/EE_SS.png", width="300" height="131">
      <div class="portfolio_text">
        <div class="paper_title" style="font-family:'Comic Sans MS'"><i>Training Dynamic Models using Early Exits for Automatic Speech Recognition on Resource-constrained Devices</i></div>
        <div class="authors"> G. A. Wright, <strong>U. Cappellazzo</strong>, S. Zaiem, D. Raj, L. Ondel Yang,  D. Falavigna, A. Brutti
        </div>
        <div class="submission_info">
          <mark style="background-color: #FFEBCD;">Self-supervision in Audio, Speech and Beyond Workshop, ICASSP 2024</mark>
        </div>
        <div class="link">
          <a href="https://arxiv.org/abs/2309.09546">Paper</a>&emsp;
          <a href="https://github.com/augustgw/early-exit-transformer">Code</a>&emsp;
        </div>
      </div>
</div>

<br>
<div>
      <img class="img-fluid" src="./images/IS_slurp.png", width="300" height="165">
      <div class="portfolio_text">
        <div class="paper_title" style="font-family:'Comic Sans MS'"><i>Sequence-Level Knowledge Distillation for Class-Incremental End-to-End Spoken Language Understanding</i></div>
        <div class="authors"> <strong>Umberto Cappellazzo</strong>, Muqiao Yang, Daniele Falavigna, Alessio Brutti
        </div>
        <div class="submission_info">
          <mark style="background-color: #CC99FF;">INTERSPEECH 2023 (Oral)</mark>
        </div>
        <div class="link">
          <a href="https://arxiv.org/abs/2305.13899">Paper</a>&emsp;
          <a href="https://github.com/umbertocappellazzo/SLURP-SeqKD">Code</a>&emsp;
        </div>
      </div>
</div>

<br>
<div>
      <img class="img-fluid" src="./images/FSC_CL.png", width="300" height="180">
      <div class="portfolio_text">
        <div class="paper_title" style="font-family:'Comic Sans MS'">An Investigation of the Combination of Rehearsal and Knowledge Distillation in Continual Learning for Spoken Language Understanding
        </div>
        <div class="authors"> <strong>Umberto Cappellazzo</strong>, Daniele Falavigna, Alessio Brutti
        </div>
        <div class="submission_info">
          <mark style="background-color: #CC99FF;">INTERSPEECH 2023 (Poster)</mark>
        </div>
        <div class="link">
          <a href="https://arxiv.org/abs/2211.08161">Paper</a>&emsp;
          <a href="https://github.com/umbertocappellazzo/CL_SLU">Code</a>&emsp;
        </div>
      </div>
</div>
<br>

<h2 id="education">Education</h2>
<ul>
  <li><span><img src="./images/unitn_logo.png" width="20px"></span>
  PhD in Information Engineering and Computer Science  <br>
  University of Trento, Trento, Italy<br>
  Nov 2021-Present<br>
  <b>Title</b>: <i>Continual Learning for audio and speech processing</i> <br>
  <b>Supervisors</b>: Daniele Falavigna, Alessio Brutti<br>
  </li>
  <br>

  <li>
    <span><img src="./images/logo_unipd.png" width="20px"></span>
  M.S. in Telecommunication Engineering<br>
  University of Padova, Padova, Italy<br>
  2016-2019<br>
  <b>Thesis title</b>: <i>A Deep Learning-Based ECG Delineator: Evaluation and Comparison on Standard Databases</i>. It can be accessed <a href="https://thesis.unipd.it/handle/20.500.12608/28912" target="_blank">here</a>. 
  <br>
  <b>Supervisors</b>: Michele Rossi, Matteo Gadaleta
  </li>
  <br>

  <li>
    <span><img src="./images/logo_unipd.png" width="20px"></span>
  B.S. in Information Engineering<br>
  University of Padova, Padova, Italy<br>
  2013-2016<br>
  <b>Thesis title</b>: <i>Message Authentication over an Ideal or Noisy Channel</i> 
  <br>
  <b>Supervisor</b>: Nicola Laurenti
  </li>
  <br>

</ul>


<h2>Contact</h2>
Feel free to reach out to me if you have any questions upon my research activity. Plus, I'm always apt to new collaborations! Please contact me at <i>umbertocappellazzo [at] gmail [dot] com</i>.
    
</br>
</br>

  </body>
</html>